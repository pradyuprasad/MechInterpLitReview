# Mechanistic Interpretability of Large Language Models: A Survey

This repository contains the LaTeX source code and materials for the paper **"Mechanistic Interpretability of Large Language Models: A Survey"**. The survey explores how mechanistic interpretability aims to uncover and explain the internal computations of large language models (LLMs) like transformers.

---

## ðŸ“œ About the Survey

Mechanistic interpretability seeks to understand the internal mechanisms of neural networks by reverse-engineering their computations. This survey:
- Introduces key concepts such as **features**, **polysemanticity**, and **circuits**.
- Reviews techniques like **probing**, **sparse autoencoders**, **ablation**, and **activation patching**.
- Discusses challenges in scaling interpretability methods to larger models and the redundancy in neural circuits.

---

## ðŸŒŸ Key Highlights

- **Key Concepts**: Explains the foundational ideas like superposition, polysemanticity, and circuits that govern neural computations.
- **Techniques**:
  - Observational: Probing and sparse autoencoders for diagnosing model representations.
  - Interventional: Ablation and activation patching to identify causal relationships.
- **Challenges**: Addresses issues like scalability, redundancy, and the need for standardized evaluation metrics.

---

## ðŸ“‚ Files

- **`paper.tex`**: Main LaTeX file for the survey.
- **`references.bib`**: Bibliography file.
- **`survey.pdf`**: Compiled version of the paper.
- **Images**: Supporting illustrations used in the paper.

